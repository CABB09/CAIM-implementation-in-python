{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Función para calcular la entropía de un conjunto de datos\n",
    "def entropy(target_col):\n",
    "    elements, counts = np.unique(target_col, return_counts=True)\n",
    "    return np.sum([(-count / np.sum(counts)) * np.log2(count / np.sum(counts)) for count in counts])\n",
    "\n",
    "# Función para calcular la ganancia de información de un atributo\n",
    "def InfoGain(data, split_attribute_name, target_name):\n",
    "    total_entropy = entropy(data[target_name])\n",
    "    vals, counts = np.unique(data[split_attribute_name], return_counts=True)\n",
    "    Weighted_Entropy = np.sum([(counts[i] / np.sum(counts)) * entropy(data.where(data[split_attribute_name] == vals[i]).dropna()[target_name]) for i in range(len(vals))])\n",
    "    return total_entropy - Weighted_Entropy\n",
    "\n",
    "# Función para predecir una clase para una nueva fila\n",
    "def predict(row, tree):\n",
    "    while isinstance(tree, dict):\n",
    "        root_node = next(iter(tree))\n",
    "        node_value = row.get(root_node, None)\n",
    "        if node_value in tree[root_node]:\n",
    "            tree = tree[root_node][node_value]\n",
    "        else:\n",
    "            # Se retorna la clase más común si el valor no está en el árbol\n",
    "            return most_common_class(tree[root_node])\n",
    "    return tree\n",
    "\n",
    "# Se encuentra la clase más común en un nodo del árbol. \n",
    "def most_common_class(node):\n",
    "    if isinstance(node, dict):\n",
    "        # Se crea una lista plana de todos los valores no-dict del árbol\n",
    "        leaf_values = []\n",
    "        def get_leaf_values(subnode):\n",
    "            if isinstance(subnode, dict):\n",
    "                for value in subnode.values():\n",
    "                    get_leaf_values(value)\n",
    "            else:\n",
    "                leaf_values.append(subnode)\n",
    "        get_leaf_values(node)\n",
    "        # Se devuelve el valor más común en la lista de valores hoja\n",
    "        return max(set(leaf_values), key=leaf_values.count)\n",
    "    return node\n",
    "\n",
    "\n",
    "# Función principal para el algoritmo ID3\n",
    "def ID3(data, originaldata, features, target_attribute_name, parent_node_class=None):\n",
    "    if len(data) == 0:\n",
    "        return parent_node_class\n",
    "    elif len(np.unique(data[target_attribute_name])) <= 1:\n",
    "        return data[target_attribute_name].iloc[0]\n",
    "    elif len(features) == 0:\n",
    "        return np.unique(data[target_attribute_name])[np.argmax(np.unique(data[target_attribute_name], return_counts=True)[1])]\n",
    "    else:\n",
    "        # Se calcula la ganancia de información para cada atributo\n",
    "        item_values = [InfoGain(data, feature, target_attribute_name) for feature in features]\n",
    "        # Se obtiene el atributo con la mayor ganancia de información\n",
    "        best_feature_index = np.argmax(item_values)\n",
    "        best_feature = features[best_feature_index]\n",
    "        # Se construye el árbol\n",
    "        tree = {best_feature: {}}\n",
    "        # Se elimina el atributo con la mayor ganancia de información\n",
    "        features = [i for i in features if i != best_feature]\n",
    "\n",
    "        # Se construye un árbol por cada posible valor del atributo\n",
    "        for value in np.unique(data[best_feature]):\n",
    "            sub_data = data.where(data[best_feature] == value).dropna()\n",
    "            subtree = ID3(sub_data, data, features, target_attribute_name, np.unique(data[target_attribute_name])[np.argmax(np.unique(data[target_attribute_name], return_counts=True)[1])])\n",
    "            tree[best_feature][value] = subtree\n",
    "        return tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones para la validación cruzada\n",
    "def cross_validation_split(data, folds):\n",
    "    data_split = np.array_split(data, folds)\n",
    "    return data_split\n",
    "\n",
    "# Se define CV estratificado. Intenta mantener la proporción de datos de cada clase\n",
    "def stratified_cross_validation_split(data, folds, target_attribute_name):\n",
    "    data_split = []\n",
    "    unique_classes = np.unique(data[target_attribute_name])\n",
    "    for _ in range(folds):\n",
    "        fold = pd.DataFrame()\n",
    "        for cls in unique_classes:\n",
    "            class_data = data[data[target_attribute_name] == cls]\n",
    "            fold = pd.concat([fold, class_data.sample(frac=1/folds)])\n",
    "        data_split.append(fold)\n",
    "    return data_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se define la validación cruzada\n",
    "def cross_validate(data, folds, target_attribute_name, stratified=False):\n",
    "    if stratified:\n",
    "        splits = stratified_cross_validation_split(data, folds, target_attribute_name)\n",
    "    else:\n",
    "        splits = cross_validation_split(data, folds)\n",
    "    accuracies = []\n",
    "\n",
    "    # Se aplica un ciclo for para que realice todo el proceso de CV\n",
    "    for fold in splits:\n",
    "        train_set = pd.concat([df for df in splits if df is not fold])\n",
    "        test_set = fold\n",
    "        features = [col for col in data.columns if col != target_attribute_name]\n",
    "        tree = ID3(train_set, train_set, features, target_attribute_name)\n",
    "        predictions = test_set.apply(lambda row: predict(row, tree), axis=1)\n",
    "        accuracy = (predictions == test_set[target_attribute_name]).mean()\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Carlo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión media (simple): 0.5450934155632143\n",
      "Desviación estándar de la precisión (simple): 0.05214508850003015\n",
      "Precisión media (estratificada): 0.5687074829931973\n",
      "Desviación estándar de la precisión (estratificada): 0.02999375195244707\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"C:\\\\Users\\\\Carlo\\\\Desktop\\\\IA\\\\segundo semestre\\\\apredizaje automatico\\\\tarea\\\\base de datos discretizadas\\\\discretized_Yeast.csv\")\n",
    "data = data.drop('Sequence_Name', axis=1) \n",
    "accuracies_simple = cross_validate(data, 10, \"class\", stratified=False)\n",
    "print(\"Precisión media (simple):\", np.mean(accuracies_simple))\n",
    "print(\"Desviación estándar de la precisión (simple):\", np.std(accuracies_simple))\n",
    "accuracies_stratified = cross_validate(data, 10, \"class\", stratified=True)\n",
    "print(\"Precisión media (estratificada):\", np.mean(accuracies_stratified))\n",
    "print(\"Desviación estándar de la precisión (estratificada):\", np.std(accuracies_stratified))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
